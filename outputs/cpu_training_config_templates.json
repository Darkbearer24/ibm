{
  "small_dataset": {
    "config": {
      "device": "cpu",
      "mixed_precision": false,
      "pin_memory": false,
      "num_workers": 0,
      "batch_size": 4,
      "learning_rate": 0.0015,
      "num_epochs": 30,
      "early_stopping": {
        "enabled": true,
        "patience": 5,
        "min_delta": 0.0001,
        "restore_best_weights": true,
        "monitor": "val_loss",
        "mode": "min"
      },
      "lr_scheduler": {
        "type": "ReduceLROnPlateau",
        "params": {
          "mode": "min",
          "factor": 0.5,
          "patience": 3,
          "verbose": true,
          "min_lr": 1e-06
        }
      },
      "gradient_clipping": {
        "enabled": true,
        "max_norm": 1.0
      },
      "checkpointing": {
        "save_every": 6,
        "save_best_only": true,
        "save_last": true,
        "monitor": "val_loss",
        "mode": "min"
      }
    },
    "tips": [
      "Use smaller batch sizes (2-8) for better CPU performance",
      "Enable gradient clipping to prevent exploding gradients",
      "Use early stopping to prevent overfitting on small datasets",
      "Monitor training closely due to limited computational resources",
      "Consider data augmentation if dataset is very small",
      "Save checkpoints frequently in case of interruption"
    ],
    "expected_performance": {
      "training_time_multiplier": "10-50x slower than GPU",
      "memory_usage": "Lower than GPU training",
      "convergence": "May require more epochs for small datasets"
    }
  },
  "medium_dataset": {
    "config": {
      "device": "cpu",
      "mixed_precision": false,
      "pin_memory": false,
      "num_workers": 0,
      "batch_size": 8,
      "learning_rate": 0.001,
      "num_epochs": 20,
      "early_stopping": {
        "enabled": true,
        "patience": 10,
        "min_delta": 0.0001,
        "restore_best_weights": true,
        "monitor": "val_loss",
        "mode": "min"
      },
      "lr_scheduler": {
        "type": "ReduceLROnPlateau",
        "params": {
          "mode": "min",
          "factor": 0.5,
          "patience": 5,
          "verbose": true,
          "min_lr": 1e-06
        }
      },
      "gradient_clipping": {
        "enabled": true,
        "max_norm": 1.0
      },
      "checkpointing": {
        "save_every": 4,
        "save_best_only": true,
        "save_last": true,
        "monitor": "val_loss",
        "mode": "min"
      }
    },
    "tips": [
      "Use smaller batch sizes (2-8) for better CPU performance",
      "Enable gradient clipping to prevent exploding gradients",
      "Use early stopping to prevent overfitting on small datasets",
      "Monitor training closely due to limited computational resources",
      "Consider data augmentation if dataset is very small",
      "Save checkpoints frequently in case of interruption"
    ],
    "expected_performance": {
      "training_time_multiplier": "10-50x slower than GPU",
      "memory_usage": "Lower than GPU training",
      "convergence": "May require more epochs for small datasets"
    }
  },
  "large_dataset": {
    "config": {
      "device": "cpu",
      "mixed_precision": false,
      "pin_memory": false,
      "num_workers": 0,
      "batch_size": 16,
      "learning_rate": 0.002,
      "num_epochs": 15,
      "early_stopping": {
        "enabled": true,
        "patience": 50,
        "min_delta": 0.0001,
        "restore_best_weights": true,
        "monitor": "val_loss",
        "mode": "min"
      },
      "lr_scheduler": {
        "type": "ReduceLROnPlateau",
        "params": {
          "mode": "min",
          "factor": 0.5,
          "patience": 25,
          "verbose": true,
          "min_lr": 1e-06
        }
      },
      "gradient_clipping": {
        "enabled": true,
        "max_norm": 1.0
      },
      "checkpointing": {
        "save_every": 3,
        "save_best_only": true,
        "save_last": true,
        "monitor": "val_loss",
        "mode": "min"
      }
    },
    "tips": [
      "Use smaller batch sizes (2-8) for better CPU performance",
      "Enable gradient clipping to prevent exploding gradients",
      "Use early stopping to prevent overfitting on small datasets",
      "Monitor training closely due to limited computational resources",
      "Consider data augmentation if dataset is very small",
      "Save checkpoints frequently in case of interruption"
    ],
    "expected_performance": {
      "training_time_multiplier": "10-50x slower than GPU",
      "memory_usage": "Lower than GPU training",
      "convergence": "May require more epochs for small datasets"
    }
  },
  "usage_instructions": [
    "1. Choose configuration based on your dataset size",
    "2. Adjust batch_size based on available memory",
    "3. Modify learning_rate if training is unstable",
    "4. Increase patience for early stopping if dataset is noisy",
    "5. Monitor training curves and adjust as needed"
  ]
}